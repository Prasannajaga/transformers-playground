# -*- coding: utf-8 -*-
"""pre-training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/transformers-model-486215-a2/locations/us-central1/repositories/e517957d-1ad2-4f55-848a-c986efc8ce68
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import logging
import torch
from torch.utils.data import DataLoader, IterableDataset
from datasets import load_dataset
from transformers import GPT2Tokenizer
import shutil

# dataframe:
# uuid: 9851FFA6-08F8-4FF2-985F-9A9273C57CD3
# output_variable:
# config_str:

print(torch.cuda.is_available() , torch.version.cuda) , torch.cuda.mem_get_info() , torch.cuda.device_count()

"""Train Config here"""

# @title
from dataclasses import dataclass
from typing import Optional, Tuple

@dataclass
class TrainingConfig:

    # System
    device: str = "cuda"
    seed: Optional[int] = 42
    mixed_precision: bool = True
    num_workers: int = 4

    # Model
    n_layer: int = 6
    n_embd: int = 384
    n_head: int = 6
    block_size: int = 256
    # block_size: int = 1024
    # n_embd: int = 768
    # n_head: int = 12
    # n_layer: int = 12

    # Data
    train_batch_size: int = 64
    eval_batch_size: int = 64
    grad_accum_steps: int = 1
    validation_batch_count: int = 20


    # Optimization
    optimizer: str = "adamw"  # Supported: adamw, adam, sgd, adafactor
    lr: float = 6e-4
    betas: Tuple[float, float] = (0.9, 0.95)
    eps: float = 1e-8
    weight_decay: float = 0.1
    grad_clip_norm: float = 1.0
    use_amp: bool = True
    dropout = 0.1

    # VRAM Optimization
    enable_gradient_checkpointing: bool = False  # Trade compute for memory
    amp_dtype: str = "bfloat16"  # "float16" or "bfloat16" (bfloat16 more stable, requires Ampere+)
    clear_cache_interval: int = 500  # Clear CUDA cache every N steps (0 to disable)
    log_memory_usage: bool = True  # Log GPU memory stats


    # LR Schedule
    total_steps: int = 10000
    warmup_steps: int = 1000

    # Checkpointing - Use Vertex AI's AIP_MODEL_DIR if available
    ckpt_dir: str = os.environ.get("AIP_MODEL_DIR", "./checkpoints")
    ckpt_interval_steps: int = 5000
    save_optimizer_state: bool = True


    # Logging
    enable_logging: bool = True
    log_interval_steps: int = 100
    log_per_iteration_time: bool = False

    # Inference config here
    max_new_tokens: int = 512
    temperature: float = 0.7

    # Sampling toggles
    use_top_k: bool = True
    top_k: int = 50

    use_repetition_penalty: bool = True
    repetition_penalty: float = 1.2

    # Runtime
    use_amp: bool = True
    amp_dtype: str = "bfloat16"   # or "float16"

    stop_on_eos: bool = True

     # Cloud Storage (Vertex AI / Google Drive)
    upload_to_drive: bool = False  # True: Google Drive, False: skip Drive upload
    upload_to_gcs: bool = False  # True: Google Cloud Storage, False: skip GCS upload
    gcs_bucket_name: Optional[str] = "transformer-garage"  # GCS bucket name (optional)
    gcs_destination_blob: Optional[str] = None  # GCS blob name (optional)
    gdrive_folder_id: Optional[str] = None  # Google Drive folder ID (optional)

config = TrainingConfig()

"""Trainer Wrapper class here"""

# @title
"""
Production-quality training wrapper for a Transformer model.
* Responsible for optimizer, amp, accumulation, clipping, checkpointing, logging, LR schedule
* Optimized for low VRAM (6GB) with gradient checkpointing, configurable AMP dtype, and memory management
"""

import os
import gc
import time
import math
import sys
import json
import tempfile
from dataclasses import asdict
from datetime import datetime
from typing import Optional, Any, Dict
import torch
from torch import nn
from torch.optim import AdamW
from tqdm import tqdm

SUPPORTED_OPTIMIZERS = frozenset({"adamw", "adam", "sgd", "adafactor"})
LOGS_DIR = "src/logs"

class Trainer:

    def __init__(
        self,
        model: nn.Module,
        config: TrainingConfig,
        device: torch.device,
        tokenizer: Optional[Any] = None,
        ckpt_dir: Optional[str] = None,
    ):
        self.model = model
        self.config = config
        self.device = device if isinstance(device, torch.device) else torch.device(device)
        self.tokenizer = tokenizer
        
        # Detect Vertex AI environment
        self.is_vertex_ai = os.environ.get("AIP_MODEL_DIR") is not None
        
        # Debug: Log all Vertex AI environment variables
        if self.config.enable_logging:
            self._log_vertex_ai_env()
        
        # checkpoint directory precedence: explicit arg > config.ckpt_dir
        self.ckpt_dir = ckpt_dir or config.ckpt_dir
        if self.ckpt_dir:
            os.makedirs(self.ckpt_dir, exist_ok=True)
            
        # device placement
        self.model.to(self.device)

        # AMP setup: disable on CPU automatically
        self.use_amp = bool(self.config.use_amp and self.device.type == "cuda")
        self.amp_dtype = self._resolve_amp_dtype()
        self.scaler = torch.amp.GradScaler(enabled=self.use_amp)


        # Optimizer with parameter groups (no weight decay on bias & LayerNorm)
        self.optimizer = self._create_optimizer()

        # lr schedule state (manual cosine + warmup)
        if self.config.total_steps is None:
            # total_steps unknown: leave as None; trainer.train will attempt to infer
            self.total_steps = None
        else:
            self.total_steps = int(self.config.total_steps)
        self.warmup_steps = int(self.config.warmup_steps or 0)
        # store base lrs to scale
        self._base_lrs = [g.get("lr", self.config.lr) for g in self.optimizer.param_groups]
        # ensure initial lr matches config.lr if not set in param groups
        for g in self.optimizer.param_groups:
            if "lr" not in g or g["lr"] is None:
                g["lr"] = self.config.lr

        # bookkeeping
        self.global_step = 0  # increments after each optimizer.step()
        self._accum_counter = 0

        # timer
        self._train_start_time = None
        self._last_step_time = None

        # Loss tracking for metadata
        self._best_train_loss = float("inf")
        self._worst_train_loss = float("-inf")
        self._final_train_loss = None
        self._best_train_loss_step = 0
        self._best_val_loss = float("inf")
        self._final_val_loss = None
        self._best_val_loss_step = 0
        self._total_tokens_trained = 0
        self._total_training_time = 0.0

        # CUDA optimizations
        if self.device.type == "cuda":
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            self._clear_cuda_cache()

        # Log initialization info
        if self.config.enable_logging:
            self._log_init_info()

    def _log(self, message: str):
        if self.config.enable_logging:
            tqdm.write(message, file=sys.stderr)
    
    def _log_vertex_ai_env(self):
        """Debug logging for Vertex AI environment variables."""
        vertex_env_vars = {
            "AIP_MODEL_DIR": os.environ.get("AIP_MODEL_DIR"),
            "AIP_CHECKPOINT_DIR": os.environ.get("AIP_CHECKPOINT_DIR"),
            "AIP_TENSORBOARD_LOG_DIR": os.environ.get("AIP_TENSORBOARD_LOG_DIR"),
            "AIP_TRAINING_DATA_URI": os.environ.get("AIP_TRAINING_DATA_URI"),
            "AIP_VALIDATION_DATA_URI": os.environ.get("AIP_VALIDATION_DATA_URI"),
            "AIP_TEST_DATA_URI": os.environ.get("AIP_TEST_DATA_URI"),
        }
        
        vertex_vars_set = {k: v for k, v in vertex_env_vars.items() if v is not None}
        
        if vertex_vars_set:
            self._log("=" * 60)
            self._log("[Trainer] VERTEX AI ENVIRONMENT DETECTED")
            self._log("=" * 60)
            for key, value in vertex_vars_set.items():
                self._log(f"[Trainer] {key}: {value}")
            self._log("=" * 60)
        else:
            self._log("[Trainer] Running in local/non-Vertex AI environment")

    def _log_init_info(self):
        params = self.parameter_counts()
        self._log(f"[Trainer] Model: {params['trainable_params_m']:.2f}M trainable / {params['total_params_m']:.2f}M total ({params['trainable_percent']:.1f}%)")
        self._log(f"[Trainer] Optimizer: {self.config.optimizer.upper()} | LR: {self.config.lr:.2e} | Weight Decay: {self.config.weight_decay}")
        self._log(f"[Trainer] Checkpoint Directory: {self.ckpt_dir}")
        if self.device.type == "cuda":
            mem = self.get_memory_summary()
            self._log(f"[Trainer] GPU Memory: {mem['allocated_gb']:.2f}GB allocated | Device: {self.device}")

    def _resolve_amp_dtype(self) -> torch.dtype:
        dtype_map = {
            "float16": torch.float16,
            "fp16": torch.float16,
            "bfloat16": torch.bfloat16,
            "bf16": torch.bfloat16,
        }
        dtype_str = self.config.amp_dtype.lower()
        if dtype_str not in dtype_map:
            self._log(f"[Trainer] Warning: Unknown amp_dtype '{self.config.amp_dtype}', defaulting to float16")
            return torch.float16

        requested_dtype = dtype_map[dtype_str]

        # Validate bfloat16 support
        if requested_dtype == torch.bfloat16 and self.device.type == "cuda":
            if not torch.cuda.is_bf16_supported():
                self._log("[Trainer] Warning: bfloat16 not supported, falling back to float16")
                return torch.float16

        return requested_dtype

    def _setup_gradient_checkpointing(self):
        if not self.config.enable_gradient_checkpointing:
            return
        if hasattr(self.model, "gradient_checkpointing_enable"):
            self.model.gradient_checkpointing_enable()
            self._log("[Trainer] Enabled gradient checkpointing")
            return
        checkpointed_count = 0
        for module in self.model.modules():
            if hasattr(module, "use_checkpoint"):
                module.use_checkpoint = True
                checkpointed_count += 1
        if checkpointed_count > 0:
            self._log(f"[Trainer] Enabled gradient checkpointing on {checkpointed_count} modules")
        else:
            self._log("[Trainer] Warning: Gradient checkpointing not supported by model")

    def _clear_cuda_cache(self):
        if self.device.type != "cuda":
            return
        gc.collect()
        torch.cuda.empty_cache()

    def reset_peak_memory_stats(self):
        if self.device.type == "cuda":
            torch.cuda.reset_peak_memory_stats(self.device)

    def get_memory_summary(self) -> Dict[str, float]:
        if self.device.type != "cuda":
            return {"device": "cpu", "allocated_gb": 0, "reserved_gb": 0, "peak_gb": 0}
        return {
            "device": str(self.device),
            "allocated_gb": round(torch.cuda.memory_allocated(self.device) / (1024 ** 3), 3),
            "reserved_gb": round(torch.cuda.memory_reserved(self.device) / (1024 ** 3), 3),
            "peak_gb": round(torch.cuda.max_memory_allocated(self.device) / (1024 ** 3), 3),
        }

    def _create_optimizer(self):
        optimizer_name = getattr(self.config, 'optimizer', 'adamw').lower()

        if optimizer_name not in SUPPORTED_OPTIMIZERS:
            raise ValueError(
                f"Unsupported optimizer: '{self.config.optimizer}'. "
                f"Supported: {', '.join(sorted(SUPPORTED_OPTIMIZERS))}"
            )

        # Exclude bias and LayerNorm/Norm weights from weight decay
        decay_params = []
        no_decay_params = []
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            if name.endswith(".bias") or "layernorm" in name.lower() or "layer_norm" in name.lower() or "ln_" in name.lower():
                no_decay_params.append(param)
            else:
                decay_params.append(param)

        param_groups = [
            {"params": decay_params, "weight_decay": self.config.weight_decay},
            {"params": no_decay_params, "weight_decay": 0.0},
        ]

        # Create optimizer based on config
        if optimizer_name == "adamw":
            return AdamW(param_groups, lr=self.config.lr, betas=self.config.betas, eps=self.config.eps)
        elif optimizer_name == "adam":
            return torch.optim.Adam(param_groups, lr=self.config.lr, betas=self.config.betas, eps=self.config.eps)
        elif optimizer_name == "sgd":
            return torch.optim.SGD(param_groups, lr=self.config.lr, momentum=0.9)
        elif optimizer_name == "adafactor":
            try:
                from transformers.optimization import Adafactor
                return Adafactor(param_groups, lr=self.config.lr, relative_step=False, warmup_init=False)
            except ImportError:
                raise ImportError(
                    "Adafactor requires the 'transformers' library. "
                    "Install with: pip install transformers"
                )


    def _get_lr_factor(self, step: int) -> float:
        if self.total_steps is None or self.total_steps <= 0:
            if self.warmup_steps > 0:
                return min(1.0, float(step) / float(max(1, self.warmup_steps)))
            return 1.0

        step = min(step, self.total_steps)
        if step < self.warmup_steps and self.warmup_steps > 0:
            return float(step) / float(max(1, self.warmup_steps))

        if self.total_steps == self.warmup_steps:
            return 1.0
        progress = float(step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))
        progress = min(max(progress, 0.0), 1.0)
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    def _update_lr(self):
        factor = self._get_lr_factor(self.global_step)
        for base, group in zip(self._base_lrs, self.optimizer.param_groups):
            group["lr"] = base * factor

    def parameter_counts(self) -> Dict[str, Any]:
        total = sum(p.numel() for p in self.model.parameters())
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        pct = (trainable / total * 100.0) if total > 0 else 0.0
        return {
            "total_params": total,
            "trainable_params": trainable,
            "trainable_percent": pct,
            "total_params_m": total / 1e6,
            "trainable_params_m": trainable / 1e6,
        }

    def _track_step_metadata(self, loss: float, batch):
        self._final_train_loss = loss
        if loss < self._best_train_loss:
            self._best_train_loss = loss
            self._best_train_loss_step = self.global_step
        if loss > self._worst_train_loss:
            self._worst_train_loss = loss
        if isinstance(batch, (list, tuple)):
            batch_tensor = batch[0]
        elif isinstance(batch, dict):
            batch_tensor = batch.get("input_ids") or batch.get("inputs") or batch.get("input")
        else:
            batch_tensor = batch
        if batch_tensor is not None and hasattr(batch_tensor, 'size'):
            batch_size = batch_tensor.size(0)
            seq_length = batch_tensor.size(1) if batch_tensor.dim() > 1 else self.config.block_size
            self._total_tokens_trained += batch_size * seq_length

    def _build_progress_postfix(
        self,
        avg_loss: float,
        lr: float,
        grad_norm: Optional[float] = None,
        val_loss: Optional[float] = None,
    ) -> Dict[str, str]:
        postfix = {
            "train_loss": f"{avg_loss:.4f}",
            "lr": f"{lr:.2e}",
        }
        if grad_norm is not None:
            postfix["gnorm"] = f"{grad_norm:.2f}"
        if val_loss is not None:
            postfix["val"] = f"{val_loss:.4f}"
        if self.config.log_memory_usage and self.device.type == "cuda":
            mem = self.get_memory_summary()
            postfix["mem"] = f"{mem['allocated_gb']:.1f}GB"
        return postfix

    def _generate_training_metadata(self, checkpoint_path: str) -> Dict[str, Any]:
        params = self.parameter_counts()
        mem = self.get_memory_summary()
        avg_tokens_per_sec = 0.0
        if self._total_training_time > 0 and self._total_tokens_trained > 0:
            avg_tokens_per_sec = self._total_tokens_trained / self._total_training_time

        return {
            "model": {
                "name": self.model.__class__.__name__,
                "checkpoint_path": checkpoint_path,
                "timestamp": datetime.now().isoformat(),
                "total_params": params["total_params"],
                "trainable_params": params["trainable_params"],
            },
            "training": {
                "total_steps": self.global_step,
                "tokens_trained": self._total_tokens_trained,
                "time_seconds": round(self._total_training_time, 2),
                "tokens_per_second": round(avg_tokens_per_sec, 2),
            },
            "train_loss": {
                "best": self._best_train_loss if self._best_train_loss != float("inf") else None,
                "worst": self._worst_train_loss if self._worst_train_loss != float("-inf") else None,
                "final": self._final_train_loss,
                "best_step": self._best_train_loss_step,
            },
            "val_loss": {
                "best": self._best_val_loss if self._best_val_loss != float("inf") else None,
                "final": self._final_val_loss,
                "best_step": self._best_val_loss_step,
            },
            "system": {
                "device": str(self.device),
                "gpu_count": torch.cuda.device_count() if self.device.type == "cuda" else 0,
                "peak_memory_gb": mem.get("peak_gb", 0),
            },
            "optimizer": {
                "name": getattr(self.config, 'optimizer', 'adamw'),
                "lr": self.config.lr,
                "weight_decay": self.config.weight_decay,
                "grad_accum_steps": self.config.grad_accum_steps,
                "batch_size": self.config.train_batch_size,
                "effective_batch_size": self.config.train_batch_size * self.config.grad_accum_steps,
            },
        }

    def _save_metadata_atomically(self, metadata: Dict[str, Any], path: str):
        dir_path = os.path.dirname(path)
        os.makedirs(dir_path, exist_ok=True)
        fd, temp_path = tempfile.mkstemp(suffix='.json', dir=dir_path)
        try:
            with os.fdopen(fd, 'w') as f:
                json.dump(metadata, f, indent=2)
            os.replace(temp_path, path)
        except Exception:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
            raise

    def _save_checkpoint_metadata(self, checkpoint_path: str, checkpoint_name: str):
        base_name = os.path.splitext(checkpoint_name)[0]
        metadata_path = os.path.join(LOGS_DIR, f"{base_name}.json")
        try:
            metadata = self._generate_training_metadata(checkpoint_path)
            self._save_metadata_atomically(metadata, metadata_path)
            self._log(f"[Trainer] Saved metadata: {metadata_path}")
        except Exception as e:
            self._log(f"[Trainer] Warning: Failed to save metadata: {e}")

    def save_checkpoint(self, step: Optional[int] = None, prefix: str = "tranformer-model") -> str:
        step = self.global_step if step is None else int(step)
        if not self.ckpt_dir:
            raise ValueError("Checkpoint directory not configured.")

        self._log(f"[Trainer] Starting checkpoint save at step {step}")
        self._log(f"[Trainer] Checkpoint base directory: {self.ckpt_dir}")
        
        # Create prefix subdirectory: ckpt_dir/prefix/
        prefix_dir = os.path.join(self.ckpt_dir, prefix)
        os.makedirs(prefix_dir, exist_ok=True)
        self._log(f"[Trainer] Checkpoint prefix directory: {prefix_dir}")

        # Save config.json once (only if it doesn't exist)
        config_path = os.path.join(prefix_dir, "config.json")
        if not os.path.exists(config_path):
            config_data = asdict(self.config)
            with open(config_path, "w") as f:
                json.dump(config_data, f, indent=2)
            self._log(f"[Trainer] Saved config: {config_path}")

        # Save checkpoint: prefix/prefix_{step}.pt
        fname = f"{prefix}_{step:07d}.pt"
        path = os.path.join(prefix_dir, fname)
        payload = {
            "model_state_dict": self.model.state_dict(),
            "training_step": step,
        }

        # Save tokenizer once (only if directory doesn't exist)
        if self.tokenizer is not None:
            tokenizer_dir = os.path.join(prefix_dir, "tokenizer")
            if not os.path.exists(tokenizer_dir):
                try:
                    self.tokenizer.save_pretrained(tokenizer_dir)
                    self._log(f"[Trainer] Saved tokenizer: {tokenizer_dir}")
                except Exception as e:
                    self._log(f"[Trainer] Warning: Failed to save tokenizer: {e}")

        if self.config.save_optimizer_state:
            payload["optimizer_state_dict"] = self.optimizer.state_dict()
        if self.scaler is not None:
            try:
                payload["scaler_state_dict"] = self.scaler.state_dict()
            except Exception:
                payload["scaler_state_dict"] = None
        
        self._log(f"[Trainer] Saving checkpoint to: {path}")
        torch.save(payload, path)
        self._log(f"[Trainer] âœ“ Checkpoint saved successfully: {path}")
        
        # Upload to cloud storage (if not on Vertex AI)
        if self.config.upload_to_drive:
            self._upload_checkpoint_to_drive(path)
        if self.config.upload_to_gcs:
            self._log(f"[Trainer] Uploading checkpoint to GCS...")
            self._upload_checkpoint_to_gcs(path, step)
        
        return path

    def _upload_checkpoint_to_drive(self, local_path: str) -> None:
        try:
            from services.cloud_storage import upload_to_gdrive

            gdrive_path = upload_to_gdrive(
                local_path=local_path,
                folder_id=self.config.gdrive_folder_id,
            )
            self._log(f"[Trainer] Uploaded to Google Drive: {gdrive_path}")
        except Exception as e:
            self._log(f"[Trainer] Warning: Google Drive upload failed: {e}")

    def _upload_checkpoint_to_gcs(self, local_path: str , step: int) -> Optional[str]:
        try:
            from services.cloud_storage import upload_to_gcs

            gcs_uri = upload_to_gcs(
                local_path=local_path,
                bucket_name=self.config.gcs_bucket_name,
                destination_blob=self.config.gcs_destination_blob + f"/hf_text_{step}.pt",
            )
            self._log(f"[Trainer] Uploaded to GCS: {gcs_uri}")
            return gcs_uri
        except Exception as e:
            self._log(f"[Trainer] Warning: GCS upload failed: {e}")
            return None

    def load_checkpoint(self, path: str, map_location: Optional[torch.device] = None) -> Dict[str, Any]:
        if map_location is None:
            map_location = self.device
        data = torch.load(path, map_location=map_location)
        self.model.load_state_dict(data["model_state_dict"], strict=True)
        if "optimizer_state_dict" in data and data["optimizer_state_dict"] is not None:
            try:
                self.optimizer.load_state_dict(data["optimizer_state_dict"])
            except Exception as e:
                self._log(f"[Trainer] Warning: Failed to load optimizer state: {e}")
        if "scaler_state_dict" in data and data["scaler_state_dict"] is not None:
            try:
                self.scaler.load_state_dict(data["scaler_state_dict"])
            except Exception:
                pass
        self.global_step = int(data.get("training_step", 0))
        saved_cfg = data.get("config", None)
        if saved_cfg and self.total_steps is None:
            self.total_steps = int(saved_cfg.get("total_steps")) if saved_cfg.get("total_steps") else None
            self.warmup_steps = int(saved_cfg.get("warmup_steps", self.warmup_steps))
        self._update_lr()
        self.model.to(self.device)
        self._log(f"[Trainer] Loaded checkpoint from {path} (step {self.global_step})")
        return data

    # Validation loss estimation
    def estimate_validation_loss(self, val_dataloader, num_batches: Optional[int] = None) -> float:
        """Memory-efficient validation loss estimation."""
        self.model.eval()
        num_batches = int(num_batches or self.config.validation_batch_count)
        total_loss = 0.0
        seen = 0

        self._clear_cuda_cache()

        with torch.no_grad():
            for i, batch in enumerate(val_dataloader):
                if seen >= num_batches:
                    break
                inputs, targets = self._unpack_batch(batch)
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
                with torch.amp.autocast(enabled=self.use_amp, device_type=self.config.device, dtype=self.amp_dtype):
                    model_out = self.model(inputs)
                    loss = self._compute_loss(model_out, targets)
                total_loss += loss.detach()
                del inputs, targets, model_out, loss
                seen += 1

        self._clear_cuda_cache()
        self.model.train()
        return (total_loss / max(1, seen)).item()

    # Core training primitives
    def _unpack_batch(self, batch):
        """Standardize dataloader batch -> (inputs, targets)."""
        if isinstance(batch, (list, tuple)):
            if len(batch) == 1:
                return batch[0], batch[0]
            return batch[0], batch[1]
        if isinstance(batch, dict):
            inp = batch.get("input_ids") or batch.get("inputs") or batch.get("input")
            tgt = batch.get("labels") or batch.get("targets") or batch.get("targets_ids")
            if inp is None:
                raise ValueError("Dict batch missing input_ids/inputs key")
            if tgt is None:
                tgt = inp
            return inp, tgt
        return batch, batch

    def _compute_loss(self, logits, targets):

        if isinstance(logits, dict) and "loss" in logits:
            return logits["loss"]
        if isinstance(logits, (tuple, list)):
            logits_tensor = logits[0]
        else:
            logits_tensor = logits

        B, T, V = logits_tensor.shape
        loss = nn.functional.cross_entropy(
            logits_tensor.reshape(-1, V),
            targets.reshape(-1),
            # ignore_index=-100,
            # reduction="mean",
        )
        return loss

    def train_step(self, batch) -> Dict[str, Any]:
        """Perform forward + backward for a single batch."""
        self.model.train()
        inputs, targets = self._unpack_batch(batch)
        inputs = inputs.to(self.device, non_blocking=True)
        targets = targets.to(self.device, non_blocking=True)
        accum_steps = max(1, int(self.config.grad_accum_steps))

        with torch.amp.autocast(enabled=self.use_amp, device_type=self.config.device, dtype=self.amp_dtype):
            model_out = self.model(inputs)
            loss = self._compute_loss(model_out, targets)

        scaled_loss = loss / accum_steps
        self.scaler.scale(scaled_loss).backward()
        self._accum_counter += 1

        did_step = False
        grad_norm = None
        current_lr = None

        if self._accum_counter >= accum_steps:
            try:
                self.scaler.unscale_(self.optimizer)
            except Exception:
                pass

            if self.config.grad_clip_norm and self.config.grad_clip_norm > 0.0:
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip_norm)
                grad_norm = float(grad_norm) if hasattr(grad_norm, "item") else float(grad_norm)
            else:
                grad_norm = None

            try:
                self.scaler.step(self.optimizer)
            except Exception as e:
                self.scaler.update()
                raise e
            else:
                self.scaler.update()
                did_step = True
                self.global_step += 1
                self._update_lr()

            finally:
                self.optimizer.zero_grad(set_to_none=True)
                self._accum_counter = 0
                current_lr = float(self.optimizer.param_groups[0]["lr"])

        return {
            "loss": loss.item(),
            "did_step": did_step,
            "grad_norm": grad_norm,
            "lr": current_lr,
        }

    # Training loop with tqdm
    def train(
        self,
        train_dataloader,
        val_dataloader=None,
        epochs: Optional[int] = 1,
        max_steps: Optional[int] = None,
    ):
        """
        High-level training loop with tqdm progress bar.
        - train_dataloader: iterable of batches
        - val_dataloader: optional validation dataloader for periodic eval
        - epochs: number of epochs to run if total_steps not specified
        - max_steps: optional override for total optimizer steps
        """
        if self.config.seed is not None:
            torch.manual_seed(self.config.seed)

        # Infer total_steps
        if max_steps is not None:
            self.total_steps = int(max_steps)
        elif self.total_steps is None:
            try:
                steps_per_epoch = len(train_dataloader)
            except Exception:
                steps_per_epoch = None
            if steps_per_epoch:
                approx_total = int(math.ceil(steps_per_epoch * float(epochs) / max(1, self.config.grad_accum_steps)))
                self.total_steps = approx_total

        self._train_start_time = time.perf_counter()
        self._last_step_time = self._train_start_time
        self.model.train()

        global_step_target = None if self.total_steps is None else int(self.total_steps)

        # Create progress bar
        pbar = tqdm(
            total=global_step_target,
            initial=self.global_step,
            desc="Training",
            unit="step",
            dynamic_ncols=True,
            leave=True,
        )

        # Tracking for smoother progress bar updates
        running_loss = 0.0
        loss_count = 0
        last_val_loss = None

        stop_requested = False
        epoch = 0

        try:
            while epoch < epochs and not stop_requested:
                epoch += 1
                for batch in train_dataloader:
                    step_info = self.train_step(batch)

                    # Accumulate loss for averaging
                    running_loss += step_info["loss"]
                    loss_count += 1

                    self._track_step_metadata(step_info["loss"], batch)

                    if step_info["did_step"]:
                        avg_loss = running_loss / loss_count

                        postfix = self._build_progress_postfix(
                            avg_loss=avg_loss,
                            lr=step_info["lr"],
                            grad_norm=step_info["grad_norm"],
                            val_loss=last_val_loss,
                        )
                        pbar.set_postfix(postfix)
                        pbar.update(1)

                        # Reset running loss periodically
                        if self.global_step % 100 == 0:
                            running_loss = 0.0
                            loss_count = 0

                        # Checkpointing
                        if self.ckpt_dir and self.config.ckpt_interval_steps:
                            if self.global_step % int(self.config.ckpt_interval_steps) == 0:
                                try:
                                    self.save_checkpoint(step=self.global_step)
                                except Exception as e:
                                    self._log(f"[Trainer] Warning: Checkpoint save failed: {e}")

                        if val_dataloader is not None and self.config.validation_batch_count:
                            if self.global_step % max(1, self.config.log_interval_steps) == 0:
                                try:
                                    last_val_loss = self.estimate_validation_loss(
                                        val_dataloader,
                                        num_batches=self.config.validation_batch_count
                                    )
                                    self._final_val_loss = last_val_loss
                                    if last_val_loss < self._best_val_loss:
                                        self._best_val_loss = last_val_loss
                                        self._best_val_loss_step = self.global_step
                                    self._log(f"[Trainer] Step {self.global_step}: val_loss={last_val_loss:.4f}")
                                except Exception as e:
                                    self._log(f"[Trainer] Warning: Validation failed: {e}")

                    # Stopping condition
                    if global_step_target is not None and self.global_step >= global_step_target:
                        stop_requested = True
                        break

                # Periodic memory cleanup
                self._clear_cuda_cache()

                if stop_requested:
                    break

        finally:
            pbar.close()

        total_wall = time.perf_counter() - self._train_start_time
        self._total_training_time = total_wall

        # Final summary
        self._log(f"[Trainer] Training complete: {self.global_step} steps in {total_wall:.1f}s ({total_wall/60:.1f}min)")
        if self.device.type == "cuda":
            mem = self.get_memory_summary()
            self._log(f"[Trainer] Peak GPU memory: {mem['peak_gb']:.2f}GB")

        # Final checkpoint with metadata (skip if already saved at interval)
        already_saved = (
            self.config.ckpt_interval_steps
            and self.global_step % int(self.config.ckpt_interval_steps) == 0
        )
        if self.ckpt_dir and not already_saved:
            try:
                ckpt_name = f"final_ckpt_step_{self.global_step:07d}.pt"
                ckpt_path = self.save_checkpoint(step=self.global_step, prefix="final_ckpt")
                self._save_checkpoint_metadata(ckpt_path, ckpt_name)
            except Exception as e:
                self._log(f"[Trainer] Warning: Final checkpoint save failed: {e}")


    def state_dict(self) -> Dict[str, Any]:
        state = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scaler_state_dict": getattr(self.scaler, "state_dict", lambda: None)(),
            "global_step": self.global_step,
            "config": asdict(self.config),
        }
        return state

    def load_state_dict(self, state: Dict[str, Any]):
        self.model.load_state_dict(state["model_state_dict"])
        if "optimizer_state_dict" in state and state["optimizer_state_dict"] is not None:
            try:
                self.optimizer.load_state_dict(state["optimizer_state_dict"])
            except Exception as e:
                self._log(f"[Trainer] Warning: optimizer state load failed: {e}")
        if "scaler_state_dict" in state and state["scaler_state_dict"] is not None:
            try:
                self.scaler.load_state_dict(state["scaler_state_dict"])
            except Exception:
                pass
        self.global_step = int(state.get("global_step", 0))
        self._update_lr()

"""Attention Layers"""

# @title


class RELU_FFN(nn.Module):
    """Position-wise feed-forward network with RELU activation"""

    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(config.dropout),
        )

    def forward(self, x):
        return self.net(x)

class MQA_BLOCK(nn.Module):
    """
    Transformer block with MQA and pluggable FFN
    """

    def __init__(self, n_embd: int, n_head: int, ffn_type: str = 'relu'):
        super().__init__()
        assert n_embd % n_head == 0
        # assert ffn_type in FFN_REGISTRY, f"Unknown FFN type: {ffn_type}"

        self.attention = MultiQueryAttention(n_embd, n_head)
        self.feedForward = RELU_FFN(n_embd)

        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.attention(self.ln1(x))
        x = x + self.feedForward(self.ln2(x))
        return x


class MultiQueryAttention(nn.Module):
    """
    Multi-Query Attention (MQA)
    - Multiple Q heads
    - Single shared K and V
    """

    def __init__(self, n_embd, n_head):
        super().__init__()
        assert n_embd % n_head == 0

        self.n_head = n_head
        self.head_dim = n_embd // n_head
        self.scale = self.head_dim ** -0.5

        # Queries: per head
        self.q_proj = nn.Linear(n_embd, n_embd, bias=False)

        # Keys & Values: SINGLE shared projection
        self.k_proj = nn.Linear(n_embd, self.head_dim, bias=False)
        self.v_proj = nn.Linear(n_embd, self.head_dim, bias=False)

        self.out_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.dropout = nn.Dropout(config.dropout)

        self.register_buffer(
            "tril",
            torch.tril(torch.ones(config.block_size, config.block_size))
        )

    def forward(self, x):
        B, T, C = x.shape

        # ---- Q ----
        q = self.q_proj(x)                       # (B, T, C)
        q = q.view(B, T, self.n_head, self.head_dim)
        q = q.transpose(1, 2)                    # (B, H, T, D)

        # ---- K, V (shared) ----
        k = self.k_proj(x)                       # (B, T, D)
        v = self.v_proj(x)                       # (B, T, D)

        k = k.unsqueeze(1)                       # (B, 1, T, D)
        v = v.unsqueeze(1)                       # (B, 1, T, D)

        # ---- Attention ----
        att = (q @ k.transpose(-2, -1)) * self.scale   # (B, H, T, T)

        att = att.masked_fill(
            self.tril[:T, :T] == 0,
            float("-inf")
        )

        att = F.softmax(att, dim=-1)
        att = self.dropout(att)

        # ---- Output ----
        out = att @ v                             # (B, H, T, D)
        out = out.transpose(1, 2).contiguous()    # (B, T, H, D)
        out = out.view(B, T, C)                   # (B, T, C)

        out = self.out_proj(out)
        out = self.dropout(out)

        return out

"""Final Decode Transformer with MQA attention"""

# @title
class DecodeTransformer(nn.Module):

    def __init__(self, num_layers, n_emb, n_head, vocab_size, block_size , dropout=0.1):
        super().__init__()

        self.token_emb = nn.Embedding(vocab_size, n_emb)
        self.position_emb = nn.Embedding(block_size, n_emb)
        self.drop = nn.Dropout(dropout)
        self.transformer_blocks = nn.ModuleList([
            MQA_BLOCK(n_embd=n_emb, n_head=n_head)
            for _ in range(num_layers)
        ])

        # Init weights
        self.apply(self._init_weights)

        self.final_norm = nn.LayerNorm(n_emb)
        self.lm_head = nn.Linear(n_emb, vocab_size)
        self.lm_head.weight = self.token_emb.weight

        # Special init for residual scaling
        # for pn, p in self.named_parameters():
        #     if pn.endswith('c_proj.weight'):
        #         torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        assert T <= self.position_emb.num_embeddings, (
            f"Sequence length {T} exceeds block_size "
            f"{self.position_emb.num_embeddings}"
        )
        assert idx.max() < self.token_emb.num_embeddings

         # 1. Standard Embedding
        token_emb = self.token_emb(idx)

        # 2. Positional Embedding (Fixed indexing)
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)
        position_emb = self.position_emb(pos)

        x = self.drop(token_emb + position_emb)

        for block in self.transformer_blocks:
            x = block(x)

        x = self.final_norm(x)
        logits = self.lm_head(x)

        loss = None
        if targets is not None:
            B, T, V = logits.shape

            logits = logits.view(B * T, V)
            targets = targets.view(B * T)

            assert logits.dim() == 2
            assert targets.dim() == 1
            assert logits.size(0) == targets.size(0)

            loss = F.cross_entropy(
                logits,
                targets,
                # ignore_index=-100,
                label_smoothing=0.0
            )

        return logits, loss

"""Start Training here"""

# @title
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Logging
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger(__name__)

# Suppress verbose HTTP logs from HuggingFace libraries
logging.getLogger("datasets").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("filelock").setLevel(logging.WARNING)
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Tokenizer
# =============================================================================
tokenizer = GPT2Tokenizer.from_pretrained("gpt2", local_files_only=False)
tokenizer.pad_token = tokenizer.eos_token
vocab_size = tokenizer.vocab_size

# Dataset
# =============================================================================
dataset = load_dataset("roneneldan/TinyStories")
dataset = dataset["train"].train_test_split(test_size=0.01, seed=config.seed)

class PackedIterableDataset(IterableDataset):
    def __init__(self, dataset, tokenizer, block_size):
        self.dataset = dataset
        self.tokenizer = tokenizer
        self.block_size = block_size

    def __iter__(self):
        buffer = []
        for text in self.dataset["text"]:
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            buffer.extend(tokens)
            buffer.append(self.tokenizer.eos_token_id)

            while len(buffer) >= self.block_size + 1:
                chunk = buffer[: self.block_size + 1]
                buffer = buffer[self.block_size + 1 :]
                yield (
                    torch.tensor(chunk[:-1], dtype=torch.long),
                    torch.tensor(chunk[1:], dtype=torch.long),
                )

train_ds = PackedIterableDataset(dataset["train"], tokenizer, config.block_size)
val_ds   = PackedIterableDataset(dataset["test"], tokenizer, config.block_size)

train_loader = DataLoader(
    train_ds,
    batch_size=config.train_batch_size,
    num_workers=config.num_workers,
    pin_memory=True,
)

val_loader = DataLoader(
    val_ds,
    batch_size=config.eval_batch_size,
    num_workers=config.num_workers,
    pin_memory=True,
)


RESUME_FROM_CHECKPOINT = False
RESUME_CKPT_PATH = os.path.join(
    config.ckpt_dir,
    "ckpt_step_0015000.pt"
)

# Model
# =============================================================================
model = DecodeTransformer(
    num_layers=config.n_layer,
    n_emb=config.n_embd,
    n_head=config.n_head,
    block_size=config.block_size,
    vocab_size=vocab_size,
).to(device)


# Trainer
# =============================================================================
trainer = Trainer(
    model=model,
    config=config,
    device=device,
    tokenizer=tokenizer,
    ckpt_dir=config.ckpt_dir,
)

# =============================================================================
# Resume training (if enabled)
# =============================================================================
if RESUME_FROM_CHECKPOINT:
    if not os.path.isfile(RESUME_CKPT_PATH):
        raise FileNotFoundError(
            f"Checkpoint not found: {RESUME_CKPT_PATH}"
        )

    log.info(f"Resuming training from checkpoint: {RESUME_CKPT_PATH}")
    trainer.load_checkpoint(RESUME_CKPT_PATH)
    log.info(f"Resumed at global_step = {trainer.global_step}")


log.info(f"Model: {trainer.parameter_counts()}")


# Train
# =============================================================================
trainer.train(
    train_dataloader=train_loader,
    val_dataloader=val_loader,
    epochs=1,  # ignored if total_steps reached
)

torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(config.device)

# torch.cuda.memory_allocated(config.device)