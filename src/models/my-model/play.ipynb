{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d37eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "/tmp/ipykernel_7166/3601310835.py:7: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "from utils import UnslothWrapper\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c785fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"data/prasanna_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8056e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading meta-llama/Llama-3.2-1B-Instruct...\n",
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 950/950 [00:00<00:00, 16627.19 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 950/950 [00:01<00:00, 747.43 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n",
      "üöÄ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Prasanna-llama-1B\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LOAD_IN_4BIT = False\n",
    "\n",
    "print(f\"‚è≥ Loading {MODEL_NAME}...\")\n",
    "model, tokenizer = UnslothWrapper.load_model_and_tokenizer(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"language\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "\n",
    "model = UnslothWrapper.get_peft_model(\n",
    "    model=model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "# dataset loading \n",
    "dataset = load_dataset(\"json\", data_files=OUTPUT_PATH, split=\"train\")\n",
    "dataset = UnslothWrapper.format_chat_dataset(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    messages_field=\"messages\",\n",
    "    output_field=\"text\",\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "TRAIN_ARGS = {\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 5,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"output_dir\": \"outputs/unsloth-sft\",\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"bf16\": bf16,\n",
    "    \"fp16\": not bf16,\n",
    "}\n",
    "\n",
    "trainer = UnslothWrapper.create_sft_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args_kwargs=TRAIN_ARGS,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting Training...\")\n",
    "# UnslothWrapper.train(trainer=trainer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d77893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smollLLM-sft-adapters/tokenizer_config.json',\n",
       " 'smollLLM-sft-adapters/special_tokens_map.json',\n",
       " 'smollLLM-sft-adapters/chat_template.jinja',\n",
       " 'smollLLM-sft-adapters/vocab.json',\n",
       " 'smollLLM-sft-adapters/merges.txt',\n",
       " 'smollLLM-sft-adapters/added_tokens.json',\n",
       " 'smollLLM-sft-adapters/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Hello  \n",
    "model.save_pretrained(\"smollLLM-sft-adapters\")\n",
    "tokenizer.save_pretrained(\"smollLLM-sft-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe04678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /home/prasanna/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `Prasanna-llama-1B`: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `Prasanna-llama-1B`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10512.04it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:31<00:00, 31.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/prasanna/coding/transformers-playground/src/models/my-model/Prasanna-llama-1B`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[unsloth_zoo.llama_cpp|ERROR]Unsloth: Error during download or introspection of original script: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py\", line 515, in _load_module_from_path\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py\", line 8813, in <module>\n",
      "    class GlmMoeDsaModel(DeepseekV2Model):\n",
      "  File \"/home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py\", line 8814, in GlmMoeDsaModel\n",
      "    model_arch = gguf.MODEL_ARCH.GLM_DSA\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: type object 'MODEL_ARCH' has no attribute 'GLM_DSA'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py\", line 560, in _download_convert_hf_to_gguf\n",
      "    module = _load_module_from_path(temp_original_file_path, original_module_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py\", line 519, in _load_module_from_path\n",
      "    raise ImportError(f\"Failed to execute module {module_name} from {filepath}\") from e\n",
      "ImportError: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: GGUF conversion failed: Failed during download/introspection of original script: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py:515\u001b[39m, in \u001b[36m_load_module_from_path\u001b[39m\u001b[34m(filepath, module_name)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    517\u001b[39m     \u001b[38;5;66;03m# Clean up registry if exec fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py:8813\u001b[39m\n\u001b[32m   8809\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._set_vocab_glm()\n\u001b[32m   8812\u001b[39m \u001b[38;5;129;43m@ModelBase\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGlmMoeDsaForCausalLM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m8813\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mGlmMoeDsaModel\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mDeepseekV2Model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   8814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgguf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_ARCH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLM_DSA\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py:8814\u001b[39m, in \u001b[36mGlmMoeDsaModel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   8812\u001b[39m \u001b[38;5;129m@ModelBase\u001b[39m.register(\u001b[33m\"\u001b[39m\u001b[33mGlmMoeDsaForCausalLM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   8813\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGlmMoeDsaModel\u001b[39;00m(DeepseekV2Model):\n\u001b[32m-> \u001b[39m\u001b[32m8814\u001b[39m     model_arch = \u001b[43mgguf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_ARCH\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLM_DSA\u001b[49m\n\u001b[32m   8815\u001b[39m     skip_mtp = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'MODEL_ARCH' has no attribute 'GLM_DSA'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py:560\u001b[39m, in \u001b[36m_download_convert_hf_to_gguf\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     module = \u001b[43m_load_module_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_original_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_module_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# Restore environment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py:519\u001b[39m, in \u001b[36m_load_module_from_path\u001b[39m\u001b[34m(filepath, module_name)\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m sys.modules[module_name]\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to execute module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[31mImportError\u001b[39m: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth/save.py:1976\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1976\u001b[39m     all_file_locations, want_full_precision, is_vlm_update = \u001b[43msave_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_sentencepiece\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass VLM flag\u001b[39;49;00m\n\u001b[32m   1985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass gpt_oss Flag\u001b[39;49;00m\n\u001b[32m   1986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth/save.py:1220\u001b[39m, in \u001b[36msave_to_gguf\u001b[39m\u001b[34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[39m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m use_local_gguf():\n\u001b[32m   1219\u001b[39m     converter_path, supported_text_archs, supported_vision_archs = (\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m         \u001b[43m_download_convert_hf_to_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1221\u001b[39m     )\n\u001b[32m   1223\u001b[39m     \u001b[38;5;66;03m# Step 3: Initial GGUF conversion\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth_zoo/llama_cpp.py:623\u001b[39m, in \u001b[36m_download_convert_hf_to_gguf\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    622\u001b[39m          \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m remove_error: logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not remove temp file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_original_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremove_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed during download/introspection of original script: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed during download/introspection of original script: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m QUANTIZATION_6_METHOD= \u001b[33m\"\u001b[39m\u001b[33mq6_k\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m QUANTIZATION_4_METHOD=\u001b[33m\"\u001b[39m\u001b[33mq4_k_m\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNEW_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mQUANTIZATION_4_METHOD\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/transformers-playground/.venv/lib/python3.12/site-packages/unsloth/save.py:1996\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1989\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1990\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: GGUF conversion failed in Kaggle environment.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1991\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is likely due to the 20GB disk space limit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1992\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTry saving to /tmp directory or use a smaller model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1993\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1994\u001b[39m         )\n\u001b[32m   1995\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: GGUF conversion failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1998\u001b[39m \u001b[38;5;66;03m# Step 9: Create Ollama modelfile\u001b[39;00m\n\u001b[32m   1999\u001b[39m modelfile_location = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: GGUF conversion failed: Failed during download/introspection of original script: Failed to execute module convert_hf_to_gguf_original_gguf_qhcrht18 from /home/prasanna/coding/transformers-playground/src/models/my-model/llama.cpp/original_gguf_qhcrht18.py"
     ]
    }
   ],
   "source": [
    "# save model \n",
    "QUANTIZATION_8_METHOD = \"q8_0\"\n",
    "QUANTIZATION_6_METHOD= \"q6_k\"\n",
    "QUANTIZATION_4_METHOD=\"q4_k_m\"\n",
    "model.save_pretrained_gguf(\n",
    "    NEW_MODEL_NAME,\n",
    "    tokenizer,\n",
    "    quantization_method = QUANTIZATION_4_METHOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a20cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Training Complete! Running Inference Test...\n",
      "<|im_start|>system\n",
      "You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.<|im_end|>\n",
      "<|im_start|>user\n",
      "tell me more about prasanna?<|im_end|>\n",
      "<|im_start|>user\n",
      "does prasanna is good developer?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Yes he is a self-taught engineer who dropped out of college and learned everything on his own through building projects and real-world experience.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Training Complete! Running Inference Test...\")\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me more about prasanna?\"}  ,\n",
    "    {\"role\": \"user\", \"content\": \"does prasanna is good developer?\"}  \n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must be True for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\") # Use \"cpu\" if you are not on GPU, but \"cuda\" is recommended for training\n",
    "\n",
    "# Generate\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 100, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"‚è≥ Loading Dataset...\")\n",
    "# dataset = load_dataset(\"json\", data_files=\"data/prasanna_data.json\", split=\"train\")\n",
    "\n",
    "# MODEL_NAME = \"HuggingFaceTB/SmolVLM-500M-Instruct\"\n",
    "# MAX_SEQ_LENGTH = 1024 # 135M models don't need massive context\n",
    "# DTYPE = None \n",
    "# LOAD_IN_4BIT = False # 135M is so small, we don't even need 4bit loading!\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = MODEL_NAME,\n",
    "#     max_seq_length = MAX_SEQ_LENGTH,\n",
    "#     dtype = DTYPE,\n",
    "#     load_in_4bit = LOAD_IN_4BIT,\n",
    "# )\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     convos = examples[\"messages\"] \n",
    "#     texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "#     return { \"text\" : texts, }\n",
    "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# len(dataset)\n",
    "# for idx in range(len(dataset)):\n",
    "#     c = formatting_prompts_func({\n",
    "#         \"messages\": [dataset[idx][\"messages\"]]\n",
    "#     })\n",
    "#     print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
