{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d37eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "/tmp/ipykernel_81754/3601310835.py:7: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import TextStreamer\n",
    "from utils import UnslothWrapper\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c785fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"data/prasanna_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8056e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading meta-llama/Llama-3.2-1B-Instruct...\n",
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 950/950 [00:00<00:00, 16627.19 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 950/950 [00:01<00:00, 747.43 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n",
      "üöÄ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Prasanna-llama-1B\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LOAD_IN_4BIT = False\n",
    "\n",
    "print(f\"‚è≥ Loading {MODEL_NAME}...\")\n",
    "model, tokenizer = UnslothWrapper.load_model_and_tokenizer(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_type=\"language\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "\n",
    "model = UnslothWrapper.get_peft_model(\n",
    "    model=model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "# dataset loading \n",
    "dataset = load_dataset(\"json\", data_files=OUTPUT_PATH, split=\"train\")\n",
    "dataset = UnslothWrapper.format_chat_dataset(\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    messages_field=\"messages\",\n",
    "    output_field=\"text\",\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "TRAIN_ARGS = {\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 5,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"output_dir\": \"outputs/unsloth-sft\",\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"bf16\": bf16,\n",
    "    \"fp16\": not bf16,\n",
    "}\n",
    "\n",
    "trainer = UnslothWrapper.create_sft_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args_kwargs=TRAIN_ARGS,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting Training...\")\n",
    "# UnslothWrapper.train(trainer=trainer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d77893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smollLLM-sft-adapters/tokenizer_config.json',\n",
       " 'smollLLM-sft-adapters/special_tokens_map.json',\n",
       " 'smollLLM-sft-adapters/chat_template.jinja',\n",
       " 'smollLLM-sft-adapters/vocab.json',\n",
       " 'smollLLM-sft-adapters/merges.txt',\n",
       " 'smollLLM-sft-adapters/added_tokens.json',\n",
       " 'smollLLM-sft-adapters/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Hello  \n",
    "model.save_pretrained(\"smollLLM-sft-adapters\")\n",
    "tokenizer.save_pretrained(\"smollLLM-sft-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe04678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "QUANTIZATION_8_METHOD = \"q8_0\"\n",
    "QUANTIZATION_6_METHOD= \"q6_k\"\n",
    "QUANTIZATION_4_METHOD=\"q4_k_m\"\n",
    "model.save_pretrained_gguf(\n",
    "    NEW_MODEL_NAME,\n",
    "    tokenizer,\n",
    "    quantization_method = QUANTIZATION_4_METHOD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a20cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Training Complete! Running Inference Test...\n",
      "<|im_start|>system\n",
      "You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.<|im_end|>\n",
      "<|im_start|>user\n",
      "tell me more about prasanna?<|im_end|>\n",
      "<|im_start|>user\n",
      "does prasanna is good developer?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Yes he is a self-taught engineer who dropped out of college and learned everything on his own through building projects and real-world experience.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Training Complete! Running Inference Test...\")\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me more about prasanna?\"}  ,\n",
    "    {\"role\": \"user\", \"content\": \"does prasanna is good developer?\"}  \n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must be True for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\") # Use \"cpu\" if you are not on GPU, but \"cuda\" is recommended for training\n",
    "\n",
    "# Generate\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 100, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"‚è≥ Loading Dataset...\")\n",
    "# dataset = load_dataset(\"json\", data_files=\"data/prasanna_data.json\", split=\"train\")\n",
    "\n",
    "# MODEL_NAME = \"HuggingFaceTB/SmolVLM-500M-Instruct\"\n",
    "# MAX_SEQ_LENGTH = 1024 # 135M models don't need massive context\n",
    "# DTYPE = None \n",
    "# LOAD_IN_4BIT = False # 135M is so small, we don't even need 4bit loading!\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = MODEL_NAME,\n",
    "#     max_seq_length = MAX_SEQ_LENGTH,\n",
    "#     dtype = DTYPE,\n",
    "#     load_in_4bit = LOAD_IN_4BIT,\n",
    "# )\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     convos = examples[\"messages\"] \n",
    "#     texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "#     return { \"text\" : texts, }\n",
    "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# len(dataset)\n",
    "# for idx in range(len(dataset)):\n",
    "#     c = formatting_prompts_func({\n",
    "#         \"messages\": [dataset[idx][\"messages\"]]\n",
    "#     })\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6429f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.767 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "HuggingFaceTB/SmolLM2-360M-Instruct does not have a padding token! Will use pad_token = <|endoftext|>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(49152, 960, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=320, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=320, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=960, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=960, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "OUTPUT_PATH = \"./datasets/prasanna_data.json\" \n",
    "NEW_MODEL_NAME = \"/home/prasanna/coding/transformers-playground/Prasanna-SmolLM-360M\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LOAD_IN_4BIT = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = NEW_MODEL_NAME, # Point to your saved folder\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None,           # Auto-detect (Float16/Bfloat16)\n",
    "    load_in_4bit = LOAD_IN_4BIT,    # Match the quantization used during training\n",
    ")\n",
    " \n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cec905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.<|im_end|>\n",
      "<|im_start|>user\n",
      "tell me more about prasanna?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Prasanna is a software engineer who is passionate about AI, AGI, and the future of work. He is based in Chennai, India and has over 3 years of experience building software solutions. He is a self-taught programmer who dropped out of college and learned everything on his own. He is proficient in Java, JavaScript, TypeScript, Rust, Python, and Go. He is based at Highperformr AI where he works in an agile environment with a focus on\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me more about prasanna?\"}  \n",
    "    # {\"role\": \"user\", \"content\": \"does prasanna is good developer?\"}  \n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must be True for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\") # Use \"cpu\" if you are not on GPU, but \"cuda\" is recommended for training\n",
    "\n",
    "# Generate\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 100, use_cache = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
