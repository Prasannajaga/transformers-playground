{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebacb073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/coding/transformers-playground/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os \n",
    "import gzip\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import IterableDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from customTransformers import DecodeTransformer \n",
    "from utils.common import save_file_text, read_file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc12f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c77612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7878",
   "metadata": {},
   "source": [
    "Mid Training QA + generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc0442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_NAMES = [\n",
    "  \"Tim\",\n",
    "  \"Tom\",\n",
    "  \"Sam\",\n",
    "  \"Bob\",\n",
    "  \"Ben\",\n",
    "  \"Max\",\n",
    "  \"Jack\",\n",
    "  \"Leo\",\n",
    "  \"Alex\",\n",
    "  \"Anna\",\n",
    "  \"Amy\",\n",
    "  \"Emma\",\n",
    "  \"Lily\",\n",
    "  \"Lucy\",\n",
    "  \"Mia\",\n",
    "  \"Ella\",\n",
    "  \"Sarah\",\n",
    "  \"John\",\n",
    "  \"Mary\"\n",
    "]\n",
    "DATASET_PATH = \"../CustomDatasets/story.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7d7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_story(story: str):\n",
    "    qas = []\n",
    "\n",
    "    sentences = story.split(\".\")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "    list = {}  \n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = s.split()\n",
    "        if len(tokens) < 3:\n",
    "            continue \n",
    "\n",
    "        name = tokens[0]\n",
    "\n",
    "        if name not in REAL_NAMES:\n",
    "            continue\n",
    "\n",
    "        if name.istitle():\n",
    "            qas.append({\n",
    "                \"q\": f\"who is {name.lower()} ?\",\n",
    "                \"a\": s.strip() + \".\"\n",
    "            })\n",
    "\n",
    "        if \"is\" in tokens or \"was\" in tokens:\n",
    "            qas.append({\n",
    "                \"q\": f\"what is {name.lower()} doing ?\",\n",
    "                \"a\": s.strip() + \".\"\n",
    "            })\n",
    "\n",
    "    return qas\n",
    "\n",
    "\n",
    "def convert_tinystories(dataset, max_samples=50_000):\n",
    "    output = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        story = ex[\"text\"].strip()\n",
    "        qa = generate_qa_from_story(story)\n",
    "\n",
    "        if len(qa) == 0:\n",
    "            continue\n",
    "\n",
    "        output.append({\n",
    "            \"story\": story,\n",
    "            \"qa\": qa\n",
    "        })\n",
    "\n",
    "        if len(output) >= max_samples:\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67215d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storyqa = convert_tinystories(ds, max_samples=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bd6b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing\n"
     ]
    }
   ],
   "source": [
    "# assert not os.path.exists(DATASET_PATH) , \"Not\"\n",
    "if not os.path.exists(DATASET_PATH): \n",
    "    print(\"Packing\")\n",
    "    save_file_text(storyqa, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48a0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "storyqa_data = read_file_text(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c04d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(storyqa_data) , storyqa_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37c073e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def normalize(text):\n",
    "    return \" \".join(text.strip().lower().split())\n",
    "\n",
    "def build_sft(): \n",
    "    data = read_file_text(DATASET_PATH)\n",
    "\n",
    "    sft = []\n",
    "    seen_pairs = set()\n",
    "\n",
    "    for ex in data:\n",
    "        for qa in ex[\"qa\"]:\n",
    "            pair_key = (\n",
    "                normalize(qa[\"q\"]),\n",
    "                normalize(qa[\"a\"])\n",
    "            )\n",
    "\n",
    "            if pair_key in seen_pairs:\n",
    "                continue\n",
    "\n",
    "            seen_pairs.add(pair_key)\n",
    "            sft.append({\n",
    "                \"prompt\": qa[\"q\"].strip(),\n",
    "                \"response\": qa[\"a\"].strip()\n",
    "            })\n",
    "\n",
    "    UNKNOWN_NAMES = [\n",
    "        \"billy\", \"alex\", \"john\", \"mark\", \"peter\",\n",
    "        \"sarah\", \"lucas\", \"james\", \"emma2\", \"tom2\"\n",
    "    ]\n",
    "\n",
    "    for name in UNKNOWN_NAMES:\n",
    "        for template in [\n",
    "            f\"who is {name} ?\",\n",
    "            f\"what is {name} doing ?\",\n",
    "            f\"tell me about {name}\"\n",
    "        ]:\n",
    "            sft.append({\n",
    "                \"prompt\": template,\n",
    "                \"response\": f\"I don't know who {name.capitalize()} is.\"\n",
    "            })\n",
    "\n",
    "    random.shuffle(sft)\n",
    "    save_file_text(sft, \"sft.json\")\n",
    "    print(f\"SFT samples: {len(sft)}\")\n",
    "    return sft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a1a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT samples: 271779\n"
     ]
    }
   ],
   "source": [
    "sftdata = build_sft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f143cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 271779)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(storyqa_data) , len(sftdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sftdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prasanna Jagadesh\n",
      "♂phone91+ 6383022025 /envel⌢peprasannnajaga9@gmail.com /linkedinlinkedin /githubgithub /gl⌢bewebsite /gl⌢beLeetcode /gl⌢behackerRank\n",
      "T echnical Skills\n",
      "Languages: Java, JavaScript, TypeScript, Rust, Python, Go\n",
      "Frameworks: Numpy, Pandas, Pytorch, Angular, React.js, Next.js, Tailwind CSS, Bootstrap, Tauri, Spring Boot, Express.js,\n",
      "FastAPI, Sequel.js, PostgreSQL, MongoDB, DynamoDB, Elasticsearch.\n",
      "Cloud & DevOps: AWS (API Gateway, Lambda, SES), GCP (Cloud Run, CloudSql, App Engine, Tasks), Git, Docker\n",
      "T ools & Concepts:Micro Services, REST APIs, Machine learning, Networking, Deep learning.\n",
      "Experience\n",
      "Highperformr AI June 2025 – Oct 2025\n",
      "Software Engineer (Full stack) Chennai, IN\n",
      "• Implemented a website tracking system similar to RB2B, allowing users to register their domains, include or exclude\n",
      "pages, and analyze de-anonymized visitor data to understand audiences and reﬁne sales strategies.\n",
      "• Designed and delivered cutting-edge, conversion-focused web experiences using Webﬂow, enhancing user\n",
      "engagement and visual appeal.\n",
      "• Scaled analytics infrastructure using ClickHouse, eﬃciently managing and querying over 700M company data\n",
      "records for real-time insights.\n",
      "• Owned the complete development lifecycle—from analyzing infrastructure costs and setting up cloud environments\n",
      "to frontend, backend, and schema design—ensuring full technical accountability.\n",
      "• Collaborated within a lean, high-performing 5-member engineering team focused on rapid delivery across frontend,\n",
      "backend, infrastructure, testing, and business requirement execution.\n",
      "Mavens-I softT ech Jan 2023 – July 2025\n",
      "Software Development Engineer (Full stack) Chennai, IN\n",
      "• Designed an intuitive and responsive UI for job searching, applications, and employer dashboards.\n",
      "• Implemented a robust REST API for job postings, user management, and application tracking.\n",
      "• Integrated serverless computing for real-time notiﬁcations and AWS SES for automated email communication.\n",
      "• Integrated serverless computing for real-time data handling using AWS Lambda with dynamodb Streams.\n",
      "• Ensured application reliability with unit and integration testing.\n",
      "• Drove major performance improvements in Angular app by optimizing change detection, lazy loading modules, and\n",
      "minimizing unnecessary re-renders.\n",
      "• Collaborated with cross-functional teams for seamless project delivery.\n",
      "• Led code reviews and ensured adherence to coding standards.\n",
      "• Used JIRA for issue tracking, sprint management, and assigning epics/stories across the dev team.\n",
      "Mavens-I softT ech Jul 2022 – Jan 2023\n",
      "Associate web Developer (UI) Chennai, IN\n",
      "• Developed scalable and high-performance web applications.\n",
      "• Created seamless user experiences with Angular & Bootstrap.\n",
      "• Built responsive and interactive UI components.\n",
      "• Integrated RESTful APIs using Spring Boot.\n",
      "Projects\n",
      "Audio-Paste – Voice-to-T ext Automation| Python, PyQt6, faster-whisper, OpenAI Whisper, pynput /githubGitHub\n",
      "• Built a desktop voice-to-text automation tool using PyQt6 and OpenAI’s Whisper model via faster-whisper SDK,\n",
      "enabling hands-free text input with automatic transcription and paste functionality.\n",
      "• Implemented global hotkey (Ctrl+Alt+R) using pynput for background voice recording, automatic silence detection,\n",
      "and seamless text insertion at cursor position without manual intervention.\n",
      "• Designed a conﬁgurable transcription pipeline supporting multiple Whisper model sizes, CPU/GPU device selection,\n",
      "and adjustable thread allocation for optimized performance across diﬀerent hardware conﬁgurations.\n",
      "Open-Tunnel - Local Tunneling Utility | Go, Go Routines, CLI /githubGitHub• Developed a fast, cross-platform command-line tool (CLI) using Go to securely expose local server ports to the\n",
      "internet (local tunneling).\n",
      "• Utilized Go Routines and the standard library’s networking capabilities for high-concurrency, low-latency persistent\n",
      "connections via WebSockets.\n",
      "• Simpliﬁed local web application development and testing by providing publicly accessible endpoints without\n",
      "requiring complex ﬁrewall or NAT conﬁgurations.\n",
      "• Created a simple, self-hosted alternative to commercial services like ngrok, oﬀering basic and eﬃcient\n",
      "local-to-public tunneling.\n",
      "Cloud Gateway Microservice | Spring Cloud Gateway, Netﬂix Eureka, Resilience4j, Redis, Express.js, FastAPI /githubGitHub\n",
      "• Implemented an API Gateway for microservices architecture using Spring Cloud Gateway, integrated with Netﬂix\n",
      "Eureka for service discovery.\n",
      "• Conﬁgured advanced resilience patterns like automatic retries, circuit breaker (Resilience4j), and fallback routes to\n",
      "ensure high availability.\n",
      "• Centralized routing, load balancing, and fault tolerance for downstream services, improving system reliability and\n",
      "scalability. Integrated Redis-based rate limiting to control traﬃc per client and protect backend services from abuse\n",
      "or overload.\n",
      "• Conﬁgured non-Java microservices built with Express.js and FastAPI using heartbeat checks and health endpoints\n",
      "for registration and monitoring in Eureka.\n",
      "FoodShare | Next.js, Node.js, Express, FastAPI, docker PostgreSQL, MongoDB, NextAuth, Tailwind, Sequelize, Tortoise/githubGitHub\n",
      "• Developed a microservices-based web application to reduce food waste by connecting donors and recipients,\n",
      "utilizing Express.js for user, post, like, comment, and claim services and FastAPI for real-time notiﬁcation services.\n",
      "• Implemented secure user authentication with session/token-based login and integrated Next.js frontend using\n",
      "NextAuth.js and rich UI components with Tailwind CSS.\n",
      "• Designed and implemented a notiﬁcation microservice using FastAPI, MongoDB, Motor, and T ortoise ORM, enabling\n",
      "asynchronous and decoupled event handling.\n",
      "• Architected the system using RESTful microservices, separating core business logic (Node/Express with PostgreSQL)\n",
      "from auxiliary services for scalability, maintainability, and independent deployment.\n",
      "• Containerized and orchestrated 6 independent microservices using Docker Compose, enabling seamless local\n",
      "development, consistent environments, and simpliﬁed service communication.\n",
      "Flow - Browser & Cmd Automater | Tauri, Rust, React.js, TypeScript, Tailwind /githubGitHub\n",
      "• Flow is a personal automation utility designed to simplify repetitive browser and command-line tasks.\n",
      "• Deﬁned custom browser mappings for URLs (e.g., open GitHub in Chrome, docs in Edge).\n",
      "• Ran custom terminal commands from predeﬁned paths.\n",
      "• Imported bookmarks from Chrome, Brave, Edge, and auto-detected and handled multiple browser proﬁles.\n",
      "Copy-Cat – Smart Clipboard Companion | Tauri, Rust, React.js, TypeScript, Tailwind /githubGitHub\n",
      "• Launched clipboard history with Ctrl + Alt + Z for instant access.\n",
      "• Automatically detected whether the copied content is regular text or source code.\n",
      "• Opens generated ﬁles directly in Visual Studio Code for seamless editing.\n",
      "ng-forms-handler | Angular, TypeScript, Reactive Forms /githubGitHub\n",
      "• Reduced boilerplate code when working with Reactive Forms in Angular, handling form initialization, validation, and\n",
      "error management.\n",
      "• Built with Angular’s Reactive Forms module, making it ideal for complex and dynamic forms.\n",
      "• Easily integrates to display meaningful error messages for invalid ﬁelds.\n",
      "Leadership & Activities\n",
      "T oastmasters Club Nov 2022 – Dec 2024\n",
      "President\n",
      "• Organized and led public speaking sessions, fostering communication and leadership among organization.\n",
      "• Planned and hosted special themed events (e.g., Debate Nights, Impromptu Battles) to increase engagement.\n",
      "• Increased member participation by 30% through active outreach, onboarding, and a fun, inclusive environment.\n",
      "• Designed promotional materials and ran social media campaigns to boost club visibility.\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../../datasets/prasanna.pdf\")\n",
    "\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eff92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prasanna.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e49841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# 1. Configuration: System Prompt and Question Mappings\n",
    "SYSTEM_PROMPT = \"You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.\"\n",
    "\n",
    "# Map resume sections to potential user questions\n",
    "SECTION_PROMPTS = {\n",
    "    \"HEADER\": [\n",
    "        \"Who is Prasanna?\",\n",
    "        \"Tell me about Prasanna.\",\n",
    "        \"Give me a summary of this candidate.\"\n",
    "    ],\n",
    "    \"SKILLS\": [\n",
    "        \"What are Prasanna's technical skills?\",\n",
    "        \"What programming languages does he know?\",\n",
    "        \"List his technical stack.\"\n",
    "    ],\n",
    "    \"EXPERIENCE\": [\n",
    "        \"Describe Prasanna's work experience.\",\n",
    "        \"What is his employment history?\",\n",
    "        \"Where has he worked previously?\"\n",
    "    ],\n",
    "    \"PROJECTS\": [\n",
    "        \"What projects has Prasanna worked on?\",\n",
    "        \"Tell me about his portfolio.\",\n",
    "        \"Describe his key projects.\"\n",
    "    ],\n",
    "    \"EDUCATION\": [\n",
    "        \"What is Prasanna's educational background?\",\n",
    "        \"Where did he go to college?\",\n",
    "        \"List his degrees.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"\n",
    "    Parses the raw text into a dictionary of sections based on common Resume Headers.\n",
    "    Assumes headers are in UPPERCASE or Title Case on their own lines.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    # Common headers to look for (adjust based on your specific txt file layout)\n",
    "    header_patterns = r\"(SKILLS|EXPERIENCE|WORK HISTORY|PROJECTS|EDUCATION|ACHIEVEMENTS|CERTIFICATIONS)\"\n",
    "    \n",
    "    # Split text by these headers\n",
    "    # The regex captures the delimiter (header) so we can use it as a key\n",
    "    parts = re.split(f\"^{header_patterns}.*$\", text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # The first part is usually the Name/Contact/Summary (Header)\n",
    "    sections[\"HEADER\"] = parts[0].strip()\n",
    "    \n",
    "    # Iterate through the rest of the parts\n",
    "    # re.split returns [text_before, header1, text_after1, header2, text_after2...]\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip().upper()\n",
    "        content = parts[i+1].strip()\n",
    "        if content:\n",
    "            sections[header] = content\n",
    "            \n",
    "    return sections\n",
    "\n",
    "def create_conversation_entry(user_query, assistant_response):\n",
    "    \"\"\"\n",
    "    Helper to format a single training example in the requested JSON structure.\n",
    "    \"\"\"\n",
    "    return [ \n",
    "        {\"role\" :\"system\" , \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "    ]\n",
    "\n",
    "def generate_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Main function to read the file and generate the dataset.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    # Step 1: Parse the text\n",
    "    sections = extract_sections(raw_text)\n",
    "\n",
    "    # Step 2: Generate entries for each section\n",
    "    for section_name, content in sections.items():\n",
    "        # Clean up content (remove excessive newlines)\n",
    "        clean_content = \" \".join(content.split())\n",
    "        \n",
    "        # Match the found section to our prompt list\n",
    "        # We try to match partial keys (e.g., \"WORK EXPERIENCE\" matches \"EXPERIENCE\")\n",
    "        matched_key = next((key for key in SECTION_PROMPTS if key in section_name), None)\n",
    "        \n",
    "        if matched_key:\n",
    "            # Generate multiple variations for better training\n",
    "            for question in SECTION_PROMPTS[matched_key]:\n",
    "                entry = create_conversation_entry(question, clean_content)\n",
    "                dataset.append({ \"messages\" : entry})\n",
    "        \n",
    "        # Special case: If it is the HEADER, ensure we have the specific \"Who is?\" question\n",
    "        if section_name == \"HEADER\":\n",
    "             entry = create_conversation_entry(\"Who is Prasanna?\", clean_content)\n",
    "             dataset.append({ \"messages\" : entry})\n",
    " \n",
    "\n",
    "    return dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e22f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 10 training examples.\n",
      "Preview of first entry:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are Prasanna's AI Assistant. You answer questions about his professional background, projects, and skills.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Who is Prasanna?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Prasanna Jagadesh \\u2642phone91+ 6383022025 /envel\\u2322peprasannnajaga9@gmail.com /linkedinlinkedin /githubgithub /gl\\u2322bewebsite /gl\\u2322beLeetcode /gl\\u2322behackerRank Technical Skills: Languages: Java, JavaScript, TypeScript, Rust, Python, Go Frameworks: Numpy, Pandas, Pytorch, Angular, React.js, Next.js, Tailwind CSS, Bootstrap, Tauri, Spring Boot, Express.js, FastAPI, Sequel.js, PostgreSQL, MongoDB, DynamoDB, Elasticsearch. Cloud & DevOps: AWS (API Gateway, Lambda, SES), GCP (Cloud Run, CloudSql, App Engine, Tasks), Git, Docker Tools & Concepts:Micro Services, REST APIs, Machine learning, Networking, Deep learning.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual file path\n",
    "input_file = \"prasanna.txt\" \n",
    "output_file = \"prasanna_dataset.json\"\n",
    "\n",
    "try:\n",
    "    data = generate_dataset(input_file)\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "    print(f\"Successfully created {len(data)} training examples.\")\n",
    "    print(f\"Preview of first entry:\\n{json.dumps(data[0], indent=2)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: prasanna.txt not found. Please ensure the file is in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831c01a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 152 entries → prasanna_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "INPUT_FILE = \"data.txt\"\n",
    "OUTPUT_FILE = \"prasanna_data.json\"\n",
    "\n",
    "\n",
    "def parse_data_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    system_match = re.match(r\"role:\\s*system,\\s*\\ncontent:\\s*(.+?)(?=\\n\\n)\", raw, re.DOTALL)\n",
    "    system_content = system_match.group(1).strip() if system_match else \"\"\n",
    "\n",
    "    blocks = re.split(r\"\\n\\d+\\n\", raw)\n",
    "    blocks = blocks[1:]\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for block in blocks:\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "\n",
    "        pairs = re.findall(\n",
    "            r\"role:\\s*(user|assistant),\\s*\\ncontent:\\s*(.+?)(?=\\nrole:|\\Z)\",\n",
    "            block,\n",
    "            re.DOTALL,\n",
    "        )\n",
    "\n",
    "        if len(pairs) < 2:\n",
    "            continue\n",
    "\n",
    "        user_content = pairs[0][1].strip()\n",
    "        assistant_content = pairs[1][1].strip()\n",
    "\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "            ]\n",
    "        }\n",
    "        dataset.append(entry)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = parse_data_file(INPUT_FILE)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Converted {len(dataset)} entries → {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
